# ğŸš€ Pipeline d'Analyse SAV Free Mobile - LLM & RAG

> Application professionnelle de traitement automatique des tweets clients via LLM (Mistral AI & Ollama) avec enrichissement RAG pour le support client.

[![Python](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.51+-red.svg)](https://streamlit.io/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

---

## ğŸ“‹ Table des matiÃ¨res
- [PrÃ©sentation](#-prÃ©sentation)
- [Installation](#-installation)
- [DÃ©marrage rapide](#-dÃ©marrage-rapide)
- [Architecture](#-architecture)
- [Configuration](#-configuration)
- [Scripts disponibles](#-scripts-disponibles)
- [Technologies](#-technologies)

---

## ğŸ¯ PrÃ©sentation

Cette application analyse automatiquement les messages clients de Free Mobile pour :
- ğŸ“Š **Classifier** les demandes (thÃ¨me, sentiment, urgence, gravitÃ©)
- ğŸ¤– **GÃ©nÃ©rer** des rÃ©ponses personnalisÃ©es via LLM
- ğŸ” **Enrichir** le contexte avec RAG (base de connaissances)
- ğŸ“ˆ **Exporter** les rÃ©sultats au format CSV standardisÃ©

### âœ¨ FonctionnalitÃ©s principales
- Interface Streamlit moderne et intuitive
- Support dual mode : **Mistral API** (cloud) ou **Ollama** (local)
- PrÃ©traitement automatique des tweets (nettoyage, dÃ©duplication)
- Enrichissement sÃ©mantique avec embeddings BERT
- Classification multi-tÃ¢ches avec LLM
- Cache intelligent pour optimiser les performances

### ğŸ“Š Cas d'usage
- DÃ©tection automatique des problÃ¨mes urgents
- Routing intelligent vers les Ã©quipes SAV
- GÃ©nÃ©ration de rÃ©ponses prÃ©-rÃ©digÃ©es
- Analyse de sentiment et KPIs clients

---

## ğŸ’¾ Installation

### PrÃ©requis
- **Python 3.10+**
- **Compte Mistral AI** avec clÃ© API ([console.mistral.ai](https://console.mistral.ai))
- **Git** (optionnel)

### Ã‰tapes d'installation

**1. Cloner le projet**
```powershell
git clone https://github.com/Imadbouchareb/LLMAnalyzer-SAV-Tweets.git
cd LLMAnalyzer-SAV-Tweets/LLM-Tweet-Pipeline
```

**2. CrÃ©er un environnement virtuel**
```powershell
python -m venv .venv
.venv\Scripts\activate
```

**3. Installer les dÃ©pendances**
```powershell
pip install -r requirements.txt
```

**4. Configurer la clÃ© API**

CrÃ©er le fichier `.streamlit/secrets.toml` :
```toml
MISTRAL_API_KEY = "votre_clÃ©_mistral_ici"
```

**5. (Optionnel) Installer Ollama pour mode local**
```powershell
# TÃ©lÃ©charger depuis https://ollama.ai
# Puis installer un modÃ¨le
ollama pull mistral
```

---

## ğŸš€ DÃ©marrage rapide

### MÃ©thode 1 : Lanceur automatique (Windows)

Double-cliquez sur **`lancer_app.bat`**

âœ… Active automatiquement l'environnement virtuel  
âœ… Lance l'application Streamlit  
âœ… Ouvre votre navigateur sur l'interface

### MÃ©thode 2 : Ligne de commande

```powershell
# Activer l'environnement
.venv\Scripts\activate

# Lancer l'application
streamlit run app.py
```

L'interface s'ouvre automatiquement sur `http://localhost:8501`

---

## ğŸ—ï¸ Architecture

```
ğŸ“¦ Pipeline de traitement
â”‚
â”œâ”€ ğŸ“¤ 1. Import & Filtrage
â”‚   â””â”€ process_tweets_pipeline.py
â”‚      â€¢ Extraction tweets clients
â”‚      â€¢ Suppression rÃ©ponses Free
â”‚      â€¢ DÃ©tection langue
â”‚
â”œâ”€ ğŸ” 2. Enrichissement RAG
â”‚   â””â”€ add_rag_context.py
â”‚      â€¢ Recherche sÃ©mantique
â”‚      â€¢ Embeddings BERT
â”‚      â€¢ Injection contexte
â”‚
â”œâ”€ ğŸ¤– 3. Analyse LLM (2 options)
â”‚   â”œâ”€ Option A: Mistral API
â”‚   â”‚   â””â”€ llm_batch_multitask_pool_mistral.py
â”‚   â”‚      â€¢ Cloud, rapide
â”‚   â”‚      â€¢ NÃ©cessite clÃ© API
â”‚   â”‚
â”‚   â””â”€ Option B: Ollama Local
â”‚       â””â”€ llm_full_ollama_pipeline.py
â”‚          â€¢ Local, gratuit
â”‚          â€¢ Sans clÃ© API
â”‚
â””â”€ ğŸ“Š 4. Export CSV
    â””â”€ Format standardisÃ© 16 colonnes
```

---

## ğŸ” Configuration

### Configuration de la clÃ© API Mistral

**MÃ©thode 1 : Streamlit Secrets (RECOMMANDÃ‰)**

CrÃ©er `.streamlit/secrets.toml` :
```toml
MISTRAL_API_KEY = "votre_clÃ©_mistral_ici"
```

**MÃ©thode 2 : Variable d'environnement**
```powershell
$env:MISTRAL_API_KEY = "votre_clÃ©_mistral_ici"
```

**MÃ©thode 3 : Fichier .env**
```bash
MISTRAL_API_KEY=votre_clÃ©_mistral_ici
```

### SÃ©curitÃ©

âš ï¸ **Important** : Ne jamais committer les fichiers contenant des clÃ©s API

Fichiers protÃ©gÃ©s dans `.gitignore` :
- `.streamlit/secrets.toml`
- `.env`
- `*.sqlite` (caches)
- `*.pt` (embeddings)

---

## ğŸ“¦ Scripts disponibles

| Script | Description | Usage principal |
|--------|-------------|-----------------|
| `app.py` | Interface Streamlit | Interface utilisateur complÃ¨te |
| `process_tweets_pipeline.py` | PrÃ©traitement | Extraction et nettoyage des tweets |
| `add_rag_context.py` | Enrichissement RAG | Injection de contexte sÃ©mantique |
| `llm_batch_multitask_pool_mistral.py` | Pipeline Mistral | Classification et gÃ©nÃ©ration (cloud) |
| `llm_full_ollama_pipeline.py` | Pipeline Ollama | Alternative locale sans API |
| `lancer_app.bat` | Lanceur Windows | DÃ©marrage automatique |

### ğŸ“‚ Structure des fichiers

```
LLM-Tweet-Pipeline/
â”œâ”€â”€ ğŸ“„ app.py                              # Interface Streamlit
â”œâ”€â”€ ğŸ“„ process_tweets_pipeline.py          # PrÃ©traitement
â”œâ”€â”€ ğŸ“„ add_rag_context.py                  # RAG
â”œâ”€â”€ ğŸ“„ llm_batch_multitask_pool_mistral.py # Pipeline Mistral
â”œâ”€â”€ ğŸ“„ llm_full_ollama_pipeline.py         # Pipeline Ollama
â”œâ”€â”€ ğŸ“„ lancer_app.bat                      # Lanceur Windows
â”œâ”€â”€ ğŸ“„ requirements.txt                    # DÃ©pendances
â”œâ”€â”€ ğŸ“Š kb_replies_rich_expanded.csv        # Base de connaissances
â”œâ”€â”€ ğŸ”§ .streamlit/secrets.toml             # Configuration API
â”œâ”€â”€ ğŸ’¾ llm_cache.sqlite                    # Cache requÃªtes
â””â”€â”€ ğŸ“ old/                                # Fichiers archivÃ©s
```

---

## ğŸ› ï¸ Technologies

### Backend
- **Python 3.10+** - Langage principal
- **Pandas** - Manipulation de donnÃ©es
- **LangChain** - Orchestration LLM
- **Mistral AI** - ModÃ¨le de langage (API)
- **Sentence-Transformers** - Embeddings sÃ©mantiques
- **PyTorch** - Calculs tensoriels

### Frontend
- **Streamlit** - Interface web interactive

### Infrastructure
- **SQLite** - Cache des requÃªtes LLM
- **Git** - Versioning
- **python-dotenv** - Gestion variables d'environnement

---

## ğŸ“Š Format de sortie

Le pipeline gÃ©nÃ¨re un CSV avec **16 colonnes standardisÃ©es** :

| Colonne | Type | Description |
|---------|------|-------------|
| `tweet_id` | str | Identifiant unique |
| `created_at_dt` | datetime | Date de publication |
| `text_display` | str | Texte du tweet |
| `rag_context` | str | Contexte RAG injectÃ© |
| `themes_list` | json | Liste des thÃ¨mes dÃ©tectÃ©s |
| `primary_label` | str | ThÃ¨me principal |
| `sentiment_label` | str | Sentiment (positif/nÃ©gatif/neutre) |
| `llm_urgency_0_3` | int | Urgence (0=faible, 3=critique) |
| `llm_severity_0_3` | int | GravitÃ© (0=mineure, 3=majeure) |
| `status` | str | Ã‰tat (open/closed) |
| `summary_1l` | str | RÃ©sumÃ© en une ligne |
| `author` | str | Auteur du tweet |
| `assigned_to` | str | Ã‰quipe responsable |
| `llm_summary` | str | RÃ©sumÃ© dÃ©taillÃ© |
| `llm_reply_suggestion` | str | RÃ©ponse suggÃ©rÃ©e |
| `routing_team` | str | Ã‰quipe de routage |

---

## ğŸ› DÃ©pannage

### ProblÃ¨me : "ClÃ© API non configurÃ©e"
**Solution :** VÃ©rifier que `.streamlit/secrets.toml` existe et contient `MISTRAL_API_KEY`

### ProblÃ¨me : "ModuleNotFoundError"
**Solution :** RÃ©installer les dÃ©pendances
```powershell
pip install -r requirements.txt
```

### ProblÃ¨me : Ollama ne rÃ©pond pas
**Solution :** 
1. VÃ©rifier qu'Ollama est installÃ© : `ollama --version`
2. VÃ©rifier qu'un modÃ¨le est tÃ©lÃ©chargÃ© : `ollama list`
3. Lancer le service : `ollama serve`

### ProblÃ¨me : Fichier CSV invalide
**Solution :** VÃ©rifier que le CSV contient au moins les colonnes `id`, `created_at`, `full_text`

---

## ğŸ“ Support

Pour toute question ou problÃ¨me :
- ğŸ“– Consultez cette documentation
- ğŸ› Ouvrez une issue sur [GitHub](https://github.com/Imadbouchareb/LLMAnalyzer-SAV-Tweets/issues)
- ğŸ“§ Contactez l'Ã©quipe de dÃ©veloppement

---

## ğŸ“„ Licence

Ce projet est dÃ©veloppÃ© pour le traitement et l'analyse automatique des demandes SAV Free Mobile.

---

<div align="center">

**ğŸ¤– Pipeline d'Analyse SAV Free Mobile - LLM & RAG**

*Application de traitement automatique avec Mistral AI & Ollama*

**Version 2.0** â€¢ Novembre 2025

[Documentation](README.md) â€¢ [Issues](https://github.com/Imadbouchareb/LLMAnalyzer-SAV-Tweets/issues) â€¢ [GitHub](https://github.com/Imadbouchareb/LLMAnalyzer-SAV-Tweets)
