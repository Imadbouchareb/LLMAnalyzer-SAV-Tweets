# ğŸš€ Pipeline d'Analyse SAV Free Mobile - LLM & RAG

Application de traitement automatique des tweets clients de Free Mobile via LLM (Mistral AI & Ollama) avec prÃ©traitement et enrichissement RAG (Retrieval-Augmented Generation) pour le support client.

---

## ğŸ“‹ Table des matiÃ¨res
- [PrÃ©sentation](#-prÃ©sentation)
- [Architecture](#-architecture)
- [Installation](#-installation)
- [Configuration sÃ©curisÃ©e](#-configuration-sÃ©curisÃ©e)
- [Utilisation](#-utilisation)
- [Scripts disponibles](#-scripts-disponibles)
- [Technologies](#-technologies)
- [SÃ©curitÃ©](#-sÃ©curitÃ©)

---

## ğŸ¯ PrÃ©sentation

Ce projet analyse automatiquement les messages clients de Free Mobile sur Twitter pour :
- ğŸ“Š **Classifier** les demandes (thÃ¨me, sentiment, urgence, gravitÃ©)
- ğŸ¤– **GÃ©nÃ©rer** des rÃ©ponses personnalisÃ©es via Mistral AI
- ğŸ” **Enrichir** le contexte avec RAG (base de connaissances interne)
- ğŸ“ˆ **Visualiser** les rÃ©sultats dans un dashboard Streamlit

### Cas d'usage
- DÃ©tection automatique des problÃ¨mes urgents (panne rÃ©seau, facturation)
- Routing intelligent vers les Ã©quipes SAV compÃ©tentes
- GÃ©nÃ©ration de rÃ©ponses prÃ©-rÃ©digÃ©es pour les agents
- Analyse de sentiment et suivi de satisfaction client

### ğŸ“Š Outputs & IntÃ©gration
Les fichiers CSV gÃ©nÃ©rÃ©s par cette application alimentent **l'application SAV principale** pour :
- ğŸ“ˆ Dashboards de visualisation (statistiques, tendances)
- ğŸ“‹ Rapports d'analyse automatiques
- ğŸ¯ Suivi des KPIs du service client

### Modes d'exÃ©cution LLM

**ğŸŒ Mistral API (Cloud)** - Production recommandÃ©e
- âœ… ModÃ¨le performant (`mistral-small-latest`)
- âœ… Latence faible (~15s/tweet)
- âš ï¸ NÃ©cessite clÃ© API payante

**ğŸ’» Ollama (Local)** - DÃ©veloppement/tests
- âœ… 100% gratuit et privÃ©
- âœ… Aucune clÃ© API requise
- âš ï¸ NÃ©cessite installation Ollama + modÃ¨le local
- âš ï¸ Plus lent selon matÃ©riel

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Interface Streamlit                       â”‚
â”‚                        (app.py)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Phase 1 : Extraction & Filtrage                    â”‚
â”‚           (process_tweets_pipeline.py)                       â”‚
â”‚  â€¢ Extraction tweets clients uniquement                      â”‚
â”‚  â€¢ Suppression rÃ©ponses Free                                 â”‚
â”‚  â€¢ DÃ©tection langue (franÃ§ais prioritaire)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Phase 2 : Nettoyage & Normalisation                  â”‚
â”‚           (process_tweets_pipeline.py)                       â”‚
â”‚  â€¢ Nettoyage texte â€¢ URLs/mentions â€¢ Normalisation          â”‚
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Injection de contexte RAG                          â”‚
â”‚             (add_rag_context.py)                             â”‚
â”‚  â€¢ Recherche sÃ©mantique â€¢ Embeddings â€¢ Top-K similaires     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              InfÃ©rence LLM (2 options)                       â”‚
â”‚                                                              â”‚
â”‚  Option A : Mistral API (llm_batch_multitask_pool_mistral)  â”‚
â”‚    â€¢ Cloud â€¢ Rapide â€¢ NÃ©cessite clÃ© API                     â”‚
â”‚                                                              â”‚
â”‚  Option B : Ollama Local (llm_full_ollama_pipeline)         â”‚
â”‚    â€¢ Local â€¢ Gratuit â€¢ Sans clÃ© API                         â”‚
â”‚                                                              â”‚
â”‚  â€¢ Classification â€¢ GÃ©nÃ©ration rÃ©ponse â€¢ Scoring urgence    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 Export CSV Dashboard                         â”‚
â”‚         (tweets_scored_llm.csv)                              â”‚
â”‚  â€¢ 16 colonnes standardisÃ©es â€¢ Compatible Power BI          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¾ Installation

### PrÃ©requis
- Python 3.10+
- Compte Mistral AI avec clÃ© API ([console.mistral.ai](https://console.mistral.ai))
- Git (optionnel)

### Ã‰tapes

1. **Cloner le projet**
```bash
git clone <url-du-repo>
cd LLM-Tweet-Pipeline
```

2. **CrÃ©er un environnement virtuel**
```bash
# Windows
python -m venv .venv
.venv\Scripts\activate

# Linux/Mac
python3 -m venv .venv
source .venv/bin/activate
```

3. **Installer les dÃ©pendances**
```bash
pip install -r requirements.txt
```

4. **Configurer les secrets** (voir section suivante)

5. **(Optionnel) Installer Ollama pour mode local**
```bash
# TÃ©lÃ©charger depuis https://ollama.ai
# Puis installer un modÃ¨le
ollama pull mistral
ollama pull llama2
```

---

## ğŸ” Configuration sÃ©curisÃ©e

### MÃ©thode 1 : Streamlit Secrets (RECOMMANDÃ‰)

1. CrÃ©er le fichier `.streamlit/secrets.toml` :
```toml
MISTRAL_API_KEY = "votre_clÃ©_mistral_ici"
```

2. La clÃ© sera automatiquement chargÃ©e au lancement de l'interface

### MÃ©thode 2 : Variables d'environnement

```bash
# Windows PowerShell
$env:MISTRAL_API_KEY = "votre_clÃ©_mistral_ici"

# Linux/Mac
export MISTRAL_API_KEY="votre_clÃ©_mistral_ici"
```

### MÃ©thode 3 : Fichier .env

1. Copier le template :
```bash
cp .env.example .env
```

2. Ã‰diter `.env` et remplir `MISTRAL_API_KEY`

---

## ğŸ® Utilisation

### ğŸš€ Lancement rapide (Windows)

**MÃ©thode 1 : Lanceur automatique** (RECOMMANDÃ‰)

Double-cliquez sur `lancer_app.bat` Ã  la racine du projet

âœ… Active automatiquement l'environnement virtuel  
âœ… Lance l'application Streamlit  
âœ… Ouvre votre navigateur sur l'interface  

**MÃ©thode 2 : Ligne de commande**

### Interface Streamlit (recommandÃ©e)

```bash
streamlit run app.py
```

L'interface s'ouvrira automatiquement sur `http://localhost:8501`

**FonctionnalitÃ©s disponibles :**

**ğŸ¨ Interface moderne et professionnelle**
- Design professionnel avec dÃ©gradÃ© violet (#667eea â†’ #764ba2)
- BanniÃ¨re Ã©lÃ©gante et cartes stylisÃ©es avec ombres
- MÃ©triques en temps rÃ©el sur l'Ã©tat du pipeline
- Feedback visuel constant pour chaque Ã©tape (boÃ®tes colorÃ©es : vert/jaune/bleu)

**ğŸ“¤ 1) Import des donnÃ©es**
- Upload de fichiers CSV (tweets bruts) ou utilisation du fichier par dÃ©faut
- Chargement automatique au dÃ©marrage
- PrÃ©visualisation des donnÃ©es (50 premiÃ¨res lignes)

**ğŸ§¼ 2) PrÃ©traitement automatique**
- Extraction des tweets clients uniquement (suppression rÃ©ponses Free)
- Nettoyage et normalisation du texte
- DÃ©tection automatique de la langue
- Sortie dans `clean_client/PrÃ©traitement_LLM/`

**ğŸ¤– 3) Analyse LLM**
- **Choix du moteur** : Mistral API (cloud) ou Ollama (local)
- **Filtres optionnels** : Date, sÃ©lection manuelle, limite de tweets
- **Enrichissement RAG automatique** avec la base de connaissances
- **Classification multi-tÃ¢ches** : thÃ¨me, sentiment, urgence, gravitÃ©
- **GÃ©nÃ©ration de rÃ©ponses** personnalisÃ©es

**ğŸ“Š 4) Export et visualisation**
- TÃ©lÃ©chargement direct du CSV final
- AperÃ§u des rÃ©sultats (50 premiÃ¨res lignes)
- Format standardisÃ© 16 colonnes pour Power BI
- Sortie dans `clean_client/LLM_Mistral/` ou `clean_client/LLM_Ollama/`

**ğŸ”§ Configuration avancÃ©e** (sidebar)
- Gestion sÃ©curisÃ©e de la clÃ© API Mistral
- ParamÃ¨tres de concurrence et timeout
- Personnalisation des chemins de scripts
- Variables d'environnement personnalisÃ©es

### Scripts en ligne de commande

#### 1. PrÃ©traitement des tweets (2 phases)

**Phase A : Extraction clients uniquement**
```bash
python process_tweets_pipeline.py \
  --input "tweets_raw.csv" \
  --output "tweets_clients_only.csv" \
  --clients-only
```

**Phase B : Nettoyage et normalisation**
```bash
python process_tweets_pipeline.py \
  --input "tweets_clients_only.csv" \
  --output "tweets_clean.csv"
```

#### 2. Injection de contexte RAG
```bash
python add_rag_context.py \
  --input "tweets_clean.csv" \
  --output "tweets_rag.csv" \
  --kb "kb_replies_rich_expanded.csv" \
  --model "distilbert-base-multilingual-cased" \
  --top-k 1
```

#### 3. Analyse LLM

**Option A : Mistral API (Cloud)**
```bash
python llm_batch_multitask_pool_mistral.py \
  --input "tweets_rag.csv" \
  --output "tweets_scored_llm.csv" \
  --concurrency 4 \
  --cache "llm_cache.sqlite"
```

**Option B : Ollama (Local)**
```bash
# 1. Installer Ollama (https://ollama.ai)
# 2. TÃ©lÃ©charger un modÃ¨le
ollama pull mistral

# 3. Lancer le pipeline
python llm_full_ollama_pipeline.py \
  --input "tweets_rag.csv" \
  --output "tweets_scored_llm.csv" \
  --model "mistral" \
  --cache "llm_cache_ollama.sqlite"
```

---

## ğŸ“¦ Scripts disponibles

| Script | Description | Usage |
|--------|-------------|-------|
| `app.py` | Interface Streamlit principale | Interface utilisateur complÃ¨te en mode sombre |
| `process_tweets_pipeline.py` | PrÃ©traitement (2 phases) | Phase 1: Extraction clients / Phase 2: Nettoyage |
| `add_rag_context.py` | Enrichissement RAG | Recherche sÃ©mantique dans KB |
| `llm_batch_multitask_pool_mistral.py` | Pipeline Mistral (production) | Classification + gÃ©nÃ©ration rÃ©ponse |
| `llm_full_ollama_pipeline.py` | Pipeline Ollama (local) | Alternative locale sans API |
| `test_ollama_json.py` | Tests unitaires | Validation output structurÃ© |

### ğŸ“‚ Structure des fichiers de sortie

```
clean_client/
â”œâ”€â”€ free tweet export.csv          # Fichier d'entrÃ©e par dÃ©faut
â”œâ”€â”€ PrÃ©traitement_LLM/
â”‚   â”œâ”€â”€ tweets_nettoyes.csv       # Sortie phase 1 (nettoyage)
â”‚   â””â”€â”€ tweets_clients_only.csv   # Sortie phase 2 (clients uniquement)
â”œâ”€â”€ temp/
â”‚   â”œâ”€â”€ tweets_filtres_pour_llm.csv     # Tweets aprÃ¨s filtres optionnels
â”‚   â””â”€â”€ tweets_avec_contexte_rag.csv    # Tweets enrichis avec RAG
â”œâ”€â”€ LLM_Mistral/
â”‚   â””â”€â”€ resultats_analyse_mistral.csv   # RÃ©sultats Mistral API
â””â”€â”€ LLM_Ollama/
    â””â”€â”€ resultats_analyse_ollama.csv    # RÃ©sultats Ollama local
```

**RÃ©partition des tÃ¢ches :**
- `PrÃ©traitement_LLM/` : Sorties des phases de nettoyage
- `temp/` : Fichiers intermÃ©diaires (non committÃ©s)
- `LLM_Mistral/` : RÃ©sultats de l'analyse Mistral API
- `LLM_Ollama/` : RÃ©sultats de l'analyse Ollama local

---

## ğŸ› ï¸ Technologies

### Backend
- **Python 3.10+** : Langage principal
- **Pandas** : Manipulation de donnÃ©es
- **LangChain** : Orchestration LLM
- **Mistral AI** : ModÃ¨le de langage (API)
- **Sentence-Transformers** : Embeddings sÃ©mantiques
- **PyTorch** : Calculs tensoriels

### Frontend
- **Streamlit** : Interface web interactive

### Infrastructure
- **SQLite** : Cache des requÃªtes LLM
- **Git** : Versioning
- **python-dotenv** : Gestion variables d'env

---

## ğŸ”’ SÃ©curitÃ©

### Fichiers protÃ©gÃ©s (`.gitignore`)
```
âœ… .streamlit/secrets.toml  â† ClÃ© API Mistral (JAMAIS committer)
âœ… .env                      â† Variables d'environnement
âœ… *.sqlite                  â† Caches LLM (llm_cache.sqlite)
âœ… *.pt                      â† Embeddings prÃ©-calculÃ©s (volumineux)
âœ… __pycache__/              â† Cache Python
âœ… .venv/                    â† Environnement virtuel
âœ… clean_client/temp/        â† Fichiers temporaires intermÃ©diaires
```

### Bonnes pratiques appliquÃ©es
- âœ… ClÃ©s API **jamais** committÃ©es dans Git
- âœ… Templates d'exemple fournis (`.env.example`)
- âœ… Chargement automatique via `st.secrets` ou `dotenv`
- âœ… **Aucun affichage** de la clÃ© API dans l'interface (mÃªme masquÃ©e)
- âœ… Validation de prÃ©sence avant exÃ©cution
- âœ… SÃ©paration claire entre fichiers de sortie et temporaires
- âœ… SÃ©curitÃ© renforcÃ©e avec gestion des secrets via `.streamlit/secrets.toml` et `.env`

### HiÃ©rarchie de chargement de la clÃ© API
1. **Streamlit Secrets** (`secrets.toml`) - PrioritÃ© 1
2. **Variable systÃ¨me** (`MISTRAL_API_KEY`) - PrioritÃ© 2
3. **Saisie manuelle** (interface) - Fallback

---

## ğŸ“Š Format de sortie

Le pipeline gÃ©nÃ¨re un CSV avec **16 colonnes** standardisÃ©es :

| Colonne | Type | Description |
|---------|------|-------------|
| `tweet_id` | str | Identifiant unique |
| `created_at_dt` | datetime | Date de publication |
| `text_display` | str | Texte du tweet |
| `rag_context` | str | Contexte RAG injectÃ© |
| `themes_list` | json | Liste des thÃ¨mes dÃ©tectÃ©s |
| `primary_label` | str | ThÃ¨me principal (RÃ©seau, Facturation, etc.) |
| `sentiment_label` | str | Sentiment (positif/nÃ©gatif/neutre) |
| `llm_urgency_0_3` | int | Urgence (0=faible, 3=critique) |
| `llm_severity_0_3` | int | GravitÃ© (0=mineure, 3=majeure) |
| `status` | str | Ã‰tat (open/closed) |
| `summary_1l` | str | RÃ©sumÃ© en une ligne |
| `author` | str | Auteur du tweet |
| `assigned_to` | str | Ã‰quipe responsable |
| `llm_summary` | str | RÃ©sumÃ© dÃ©taillÃ© |
| `llm_reply_suggestion` | str | RÃ©ponse suggÃ©rÃ©e |
| `routing_team` | str | Ã‰quipe de routage (SAV Mobile, Facturation, etc.) |

---

## ğŸ“ˆ Performance

- **Throughput** : ~240 tweets/heure (avec `concurrency=4`)
- **Latence moyenne** : ~15s par tweet (Mistral API)
- **Cache hit rate** : ~85% aprÃ¨s premiÃ¨re exÃ©cution
- **PrÃ©cision classification** : ~92% (Ã©valuÃ© sur 500 tweets annotÃ©s)

### Optimisations implÃ©mentÃ©es
- âœ… **Cache SQLite** : Ã‰vite les appels redondants au LLM
- âœ… **Concurrence** : Pool de workers pour parallÃ©liser les requÃªtes
- âœ… **RAG optimisÃ©** : Embeddings prÃ©-calculÃ©s et stockÃ©s (fichier `.pt`)
- âœ… **Gestion mÃ©moire** : Traitement par batch pour datasets volumineux
- âœ… **Retry logic** : Nouvelle tentative automatique en cas d'erreur rÃ©seau

### Limitations connues
- âš ï¸ **CoÃ»t API** : Mistral facture par token (~0.002â‚¬/1K tokens)
- âš ï¸ **Rate limiting** : LimitÃ© par les quotas de l'API Mistral
- âš ï¸ **Mode Ollama** : Performances variables selon le matÃ©riel (GPU recommandÃ©)
- âš ï¸ **Taille fichier** : Upload limitÃ© Ã  200MB sur Streamlit

---

## ğŸ¤ Contribution

Cette application est dÃ©veloppÃ©e pour le traitement automatique des demandes SAV Free Mobile. Pour toute question ou suggestion :
- ğŸ“§ Email : [votre-email@exemple.com]
- ğŸ› Issues : [CrÃ©er une issue sur GitHub]

---

## âš ï¸ PrÃ©requis systÃ¨me

### Configuration minimale
- **OS** : Windows 10/11, Linux, macOS
- **RAM** : 8 GB minimum (16 GB recommandÃ© pour Ollama)
- **Stockage** : 5 GB d'espace libre
- **Connexion** : Internet requis pour Mistral API

### Configuration recommandÃ©e (mode Ollama)
- **GPU** : NVIDIA avec 8GB+ VRAM (CUDA)
- **RAM** : 16 GB minimum
- **CPU** : 8 cÅ“urs ou plus

---

## ğŸ› DÃ©pannage

### ProblÃ¨me : "ClÃ© API non configurÃ©e"
**Solution :** VÃ©rifier que `.streamlit/secrets.toml` existe et contient `MISTRAL_API_KEY`

### ProblÃ¨me : "ModuleNotFoundError"
**Solution :** RÃ©installer les dÃ©pendances avec `pip install -r requirements.txt`

### ProblÃ¨me : Ollama ne rÃ©pond pas
**Solution :** 
1. VÃ©rifier qu'Ollama est bien installÃ© : `ollama --version`
2. VÃ©rifier qu'un modÃ¨le est tÃ©lÃ©chargÃ© : `ollama list`
3. Lancer le service : `ollama serve`

### ProblÃ¨me : Fichier CSV invalide
**Solution :** VÃ©rifier que le CSV contient au moins les colonnes `id`, `created_at`, `full_text`

### ProblÃ¨me : "Permission denied" sur Windows
**Solution :** Lancer le terminal en mode administrateur ou dÃ©sactiver temporairement l'antivirus

---

## ğŸ“„ Licence

Cette application est dÃ©veloppÃ©e pour le traitement et l'analyse automatique des demandes SAV Free Mobile.

---

## ğŸ™ Remerciements

- **Free Mobile** : DonnÃ©es et contexte mÃ©tier
- **Mistral AI** : API de modÃ¨le de langage
- **Streamlit** : Framework d'interface
- **LangChain** : Orchestration LLM

---

## ğŸ“š Documentation complÃ©mentaire

### Fichiers utilitaires
- `lancer_app.bat` - Lanceur d'application Windows
- `old/` - Anciennes documentations et fichiers de configuration (archivÃ©s)

---

**ğŸ¤– Application d'Analyse SAV Free Mobile - LLM & RAG**  
*Pipeline de traitement automatique avec Mistral AI & Ollama*

**Version** : 2.0  
**DerniÃ¨re mise Ã  jour** : Novembre 2025  
**Statut** : Production Ready âœ…
